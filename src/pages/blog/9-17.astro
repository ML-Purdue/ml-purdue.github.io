---
import "@fontsource/inter/variable.css";
import Footer from "~/components/footer.astro";
import Header from "~/components/header.astro";
import "~/styles/index.css";
import ContentSection from "~/components/content-section.astro";
import Spacer from "~/components/Spacer.astro";


const { site } = Astro;
const description = "ML@Purdue AIGuide Blog";

---

<!DOCTYPE html>
<html lang="en" class="h-full motion-safe:scroll-smooth" data-theme="dark">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
    <!-- <meta name="generator" content={generator} /> -->

    <title>ML@Purdue - AIGuide Blog</title>
    <meta name="description" content={description} />

    <!-- social media -->
    <meta property="og:title" content="ML@Purdue" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content={description} />
    <meta property="og:image" content="/social.png" />
    <meta property="og:url" content={site} />
    <meta name="twitter:card" content="summary_large_image" />
  </head>
  <body
    class="h-full overflow-x-hidden bg-default text-default text-base selection:bg-secondary selection:text-white"
  >
    <Header fixed />
    <Spacer y={96} />
        <article class="mx-auto mt-20 w-[65vw] max-w-[120ch]">
        <ContentSection id="aiguide-interviews-header" title="Computer Vision with Aref Malek">
            <h1 class="font-bold text-xl">September 17, 2023</h1>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/qfpsBDeLVVU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            <h1 class="font-bold text-xl">Transcript</h1>
            <div class="max-h-80 overflow-y-scroll w-[50vw] text-xs" style="color-scheme: dark;">
                <p>Aref is a Purdue CS major/math minor senior and the VP of the ML@Purdue club. He is
                    interested in computer vision and have pursued them through personal projects and internships. </p>
                <p>&nbsp;</p>
                <p>Note: You don&rsquo;t need to read the entire transcript or watch the entire video.
                        If you see an interesting question, you can just jump to it. Additionally there are some points in the video
                        where there were background noises so you may notice "jumps".</p>
                <p>&nbsp;</p>
                <p>Resources by Aref</p>
                <ul class="list-disc list-inside">
                    <li>D2L.ai (<a href="https://d2l.ai/">https://d2l.ai/</a>)&nbsp;interesting since Jinen in a previous interview also recommended this!</li>
                    <li>Andrej Karpathy (<a href="https://www.youtube.com/@AndrejKarpathy">https://www.youtube.com/@AndrejKarpathy</a>)&nbsp;has
                            a really helpful youtube channel</li>
                </ul>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p>&nbsp;Hi, my name is Brian and I&#39;m interviewing
                        Aref, the vice president of the ML@Purdue Club</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;Hey, good to meet you. Sup, Brian? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Hi, can you explain your background?</p>
                <p>&nbsp;</p>
                <p>*** Weird noise so had to cut out </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Background as far as stuff goes, I really liked
                        Computer Vision for a while. Jacob and I both were both into that when we started like just talking and
                        becoming friends. I think my first like four into it was with a club called AMP, Autonomous Motors Sports at
                        Purdue. They had a data mining course for it where one of the graduate students on the team was like,
                        I&#39;ll like mentor a bunch of younger kids and you know, let them do their little research or try little
                        projects and figure it out. So I did that for a while and I really liked it. So much so that I thought I was
                        going to be a vision person for sure. I worked after that, like professionally, I worked at NASA where I
                        didn&#39;t do pure computer vision I worked with a bunch of like Google Cloud AI tools. Basically try to put
                        together a project that they could use internally. In the fall, in the second semester of my sophomore year,
                        I worked on a project called VEX. It was like VEX Robotics. Right now, Nick, I believe is the lead for that.
                        Basically, it was just like a robotics competition that&#39;s done around the country every year where
                        pretty much you try to just pick up rings or some sort of objective and what&#39;s special about it, I guess
                        the semester that we did it, is that we were actually using a purely like a ML approach, meaning that there
                        was no deterministic algorithm. Like if I see the ring, then I&#39;ll go forward and pick it up. We were
                        trying to take a completely deep learning approach. So I did that throughout the spring of my sophomore year
                        and learned a lot more about computer vision. There was a little bit of reinforcement learning involved as
                        well. I didn&#39;t touch that side as much, but that&#39;s just some stuff that we took a plate around with.
                        Heading into that summer, I, very diverse background, but I&#39;ll just keep going. That summer, I worked as
                        a software intern at Amazon. I worked actually on a computer vision product, but I was a purely software
                        dev. I was working like on the back end for that machine. I wrote a little bit of like systems code, you
                        could say. And then after that, I had a little bit more research experience. I worked with Professor Bera in
                        the Ideas Lab. I worked under a visiting student at the time on a project that was like a speech to facial,
                        like a facial motion synthesis. And I&#39;ll explain what that means later. But I worked on that for a
                        couple of months before I spent this past summer at NASA again in Virginia, where I worked on a project for
                        like wildfires. Now I work as a full stack developer at Tesla for the Supercharging Network. So very long,
                        but we&#39;ll piece that apart as time goes along.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Yeah, so like I had like questions about, you know, a
                        little more, you know, in-depth detail of your background. So I see that you participated in the autonomous
                        motor sports club. So can you just explain what that club does? And, you know, your specific project?
                    </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;Yeah, I mentioned a little bit at the beginning,
                        but let me like start from the top. So AMP stands for Autonomous Motorsports Purdue. It specifically is a
                        club that works on a race car that&#39;s used at the Indy 500. And what that means is that not like the
                        actual, like, nationally televised Indy 500. There&#39;s a thing called the Indy Autonomous Challenge. And
                        what was special about that project is that cars will pretty much race around that like 500 track with
                        obstacles and cones and things like that. But what&#39;s special is that it was entirely AI based. People
                        use different approaches, right? Some people would hard-code it. Some people would try to like use deep
                        learning. But what was special about it at its core was that nobody was actually touching the car when it
                        moved. All the maneuvers that it did, all the racing that it did was completely by itself. AMP, as far as I
                        was concerned, was like a, was a VIP project from Purdue&#39;s Datamine, which I believe many first and
                        second years take apart of. At the time, what I worked on there is that they were trying to figure out like,
                        can we use a network, like just a one-and-done network to pretty much predict where the car is going and
                        what it&#39;s going to do. So what we worked on was that given some environment that we make, let&#39;s say
                        that we have a model of what the actual racetrack looks like in Unity or some sort of VR environment, can we
                        train the network that&#39;s in this little VR environment and actually successfully routes around? I worked
                        a lot on the vision aspect of that. I learned a lot because I&#39;d started at zero, right? So it was a
                        really rough start and then we got a little bit better over time. That&#39;s the general overview. I worked
                        on that as like a VIP student for a while and then for a couple of weeks, not too long, I was just talking
                        with and tried to help out the actual graduate team that works there. The guy that I did that with
                        originally is now like the team lead of the entire Purdue team and he&#39;s like a senior. So he&#39;s doing
                        very good for himself.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so I saw this YouTube video about race cars. And
                        so I have very limited knowledge on it, but I know that for race cars around turns, you want to go slow,
                        right? And there&#39;s like an optimal sort of planning. So did you incorporate that somehow into your
                        predictive algorithm? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Actually, yeah, that&#39;s a big thing that you
                        struggle with when you&#39;re collecting data for these networks, especially from a vision point of view. If
                        you just take the naive approach of like, okay, here&#39;s a car. I&#39;m at the wheel as a person driving,
                        I&#39;m just going to predict how much do I turn the wheel and how much am I going to press the gas pedal.
                        Because I mean, you&#39;re certainly not going to slow down a bunch when you&#39;re driving, right?
                        You&#39;re just everything on the slow as you don&#39;t fly up the edge, but you&#39;re trying to go fast at
                        the end of the day. So anyways, if you took the naive approach of just predicting those two things when
                        you&#39;re driving, what happens is that because the car in general is just going straight and then just
                        trying to turn at specific points, right? The roundabout, or I guess I&#39;d call it the turn. What happens
                        is that when you actually train the network, it&#39;s pretty much only used to like, if it averages out
                        where it&#39;s turning, it&#39;s kind of like slightly going to the left, right? Not like a sharp turn
                        that&#39;s usually made as you round the corner, but it averages it out just like something in the middle.
                        And that sucks because when you actually drive, you&#39;re in a situation where I&#39;m going to go, I&#39;m
                        going to go straight, I&#39;m going to go closer and closer to the edge of the curb, and then I&#39;m going
                        to completely fly off, right? So anyways, when we were designing like our data, like when we actually like
                        built the data that it was going to train on, we had to make sure that the car was able to like swerve off
                        the lane. We built in like situations where the car would drive, fly off the lane, and we punished it for
                        it, obviously. And we would also start to reward the car for following a series of points. You could imagine
                        when you&#39;re driving on the road, you pretty much predict like, either I&#39;m going to syay straight on
                        the highway, this lane got backed up or it&#39;s closing off, so I&#39;m going to change my path to merge
                        onto a different lane, something like that, right? Pretty much we had to think about how the where the car
                        is going to be, rather than how are we actually going to move the car at this instant. </p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so when you did your, you know, when you like
                        set up your training data, did you like already pre-compute like the optimal path? And then that is what the
                        virtual car is going to follow? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah, so we would, long story short, pretty much we
                        would like have a set of different driving styles. One where it would wave back and forth, one where it
                        would take wide turns, narrow turns, we would vary the whole mix, right? And what we do is that for any
                        point in time that the car was at, this car would actually be looking like maybe 10 seconds in the past,
                        right? And the data that it has or like the training data is like, where was the car at every single second,
                        10 seconds from now? So let&#39;s say that I was making that, that was that turn that you said, right? So
                        that means that at second one, I&#39;m here at second two, I&#39;m a little bit further actually closer to
                        the grass, meaning I&#39;m like closer on the turn, which means I lower my centripetal force, right? And
                        something like that along the turn so that when the car goes wide or something like that, in the actual
                        training, we would know to punish the car because it didn&#39;t follow the path that we set out. Yeah, so we
                        gave it paths to train on pretty much.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p>&nbsp;so is there like, like an already existing sort
                        of optimal path algorithm out there? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> There actually is. We were trying to keep up with this
                        one paper from NVIDIA, I think called PilotNet. And what they did is pretty much at any given moment, the
                        network will like cars here, it has like four different like paths open to it and it&#39;s basically just
                        waiting the values of each one. So one network pretty much did the job of like, here&#39;s your eight paths
                        or something. The second network said, here&#39;s the optimality of each, and then the car would choose
                        where to go from there.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so I wanted to move on to your NASA intern
                        experience. So I saw that you worked on wildfire localization and can you go more into detail about, you
                        know, the specific algorithm you use, what, you know, challenges there were with the problem, etc.
                </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;Yeah, I won&#39;t talk too much about the
                        algorithms. I&#39;m supposed to be working on it still</p>
                <p>That&#39;s a whole different conversation. But anyways, I&#39;ll give you the
                        overview of the problem. There&#39;s an existing paper already that pretty much says like, NASA had access
                        to these drones, they were basically like from the army, they were called predator drones, they&#39;re used
                        for very questionable things and stuff like that.</p>
                <p>&nbsp;</p>
                <p>*** Weird noise so had to cut out</p>
                <p>&nbsp;</p>
                <p>The reason why it&#39;s important is that there&#39;s this big drone that can fly
                        super high in the sky and it&#39;s huge. This thing is like maybe like a bigger than a school bus in length
                        and super wide. And so what&#39;s special about that is that you can put a whole bunch of sensing data. So
                        like the mid 2000s, up until like the mid 2010s, when there was fires in California, NASA was allowed to put
                        like this thing called the AMS sensor, it&#39;s like autonomous modular sensor. And so what it did is that
                        when you put it on, it would have not only access to like general camera, like your iPhone type camera,
                        you&#39;d have access to infrared readings, you would have access to thermal readings, you would have access
                        to a bunch of like different sensors. And so what&#39;s special about that is that you basically have this
                        really good problem set of like, here&#39;s all the fires in California at different times of day, and
                        different types of weather. And so regardless what the weather is like, whatever, regardless of what the
                        lighting is like, we can tell what fire looks like, right? Because we can tell that, okay, when it&#39;s
                        infrared, even if it&#39;s cloudy or something, we can still see the heat rays make it to the sensor. No one
                        ever took a deep learning approach to that. And so the paper that was done by interns about two years ago,
                        was the first to say like, Hey, here&#39;s the suite of stuff, let&#39;s make a bunch of networks that are
                        light, that are super lightweight, that can fly on like a less solidified drone, but still get the job done.
                        So what that means is that like, even if we don&#39;t get access to like a super high, fancy done, 
                </p>
                <p>&nbsp;</p>
                <p>*** Weird noise so had to cut out</p>
                <p>&nbsp;</p>
                <p>we could still get these networks benchmark them for whichever one&#39;s the best and
                        then fly it on the drone and collect the data. The reason why that&#39;s important is that right now, the
                        way that the forest system does it is that whenever there&#39;s a fire, they fly some like fire spotting
                        drones over it, they label the data, and then they&#39;re they&#39;re ready to say like, here&#39;s where
                        the fire is in like 12 hours, which is problematic, right? It&#39;s a little bit of a slow response. So our
                        dream, so our dream with the thing is that we want to be able to have a lighter drone, right? It might not
                        have the full sensor suite, but whatever it has, we load our like our network onto it. It flies over
                        wherever the fire is, it does our little detection, and then at least you have a 80% of the way to the
                        solution, right? So that even though you&#39;ll have to have someone in the loop to label it, you won&#39;t
                        have to have a 12 hour turnover loop. That&#39;s something that we really want to improve.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p>&nbsp;You mentioned that this drone uses a hyper
                        spectral like sensor or something, uses many different types of, you know, sensors like infrared, I guess,
                        like RGB too. My question is, is it possible if you could maybe just hard code this? Like, I&#39;m not
                        really sure. But like, if the thermal rating is above X, then there&#39;s a fire. So what would be the
                        advantage of using deep learning? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;So the reason why is because it&#39;s really hard
                        to capture. It&#39;s actually a very good thing that you brought that up. A lot of the existing papers
                        pretty much do that. There&#39;s another thing called the Landsat satellite. And it&#39;s this big satellite
                        that NASA sends into space. And it pretty much has almost the same suite of sensors. And you have to think
                        about how. expensive that is to fly that in space. Basically, what people do is that they come up with like
                        hard coded algorithms, like you said, where it&#39;s like, if the ratio between the thermal at this area and
                        like the IR this area is above 0.72, then that&#39;s one of the factors that may lead to fire. The problem
                        is that you&#39;ll never get it perfect, right? Because if it was perfect to get it, then there wouldn&#39;t
                        be a space or if there was a way to get it perfectly, then we wouldn&#39;t have a 12 hour turnover time to
                        detect fire, right? Yeah, let&#39;s just think about your example, right? Let&#39;s say that if thermal was
                        high enough and IR was there, then we&#39;re going to say that there&#39;s a fire. Now let&#39;s think about
                        it&#39;s a hot day in Texas, we&#39;re flying over, there&#39;s going to be a fire, but we also fly over
                        like Houston, right? Houston&#39;s hot, the sun&#39;s bright, and that means that sun is reflecting off the
                        ground, that asphalt, like the highways, is hot as hell, right? So that means that it&#39;s going to pick up
                        high IR and high thermal. Even though if I looked at it, right? It&#39;s just black asphalt. That&#39;s an
                        example, right? Because that was one of our false positives. When we try that algorithm, when we looked at
                        parts of Southern California, it would mark roads as fires very often actually. </p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so I guess this deep learning approach is a more
                        general approach? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah, also the important part about it is that if you
                        can generalize for RGB, like saying like, okay, when I have RGB like this, I tend to have IR and thermal
                        like this, right? And that means that I&#39;m very likely to predict that this is a fire. Let&#39;s say that
                        it was a pretty cloudy, like it was very smoky when I flew over the, when I flew over Southern California,
                        but I saw these red patches and when I looked at the IR and the thermal, it gave me indicators that there
                        was fire and the label was fire. So that means that even when we fly it, this is actually one of our
                        hypotheses, is that like even when we fly it without access to IR and thermal, because the RGB is associated
                        with that outcome, which is fire, we would still have a strong predictor for it</p>
                <p>That&#39;s something we have to test and that&#39;s something why I have to keep
                        working. </p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, my next question So I saw that</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;So let me know if that doesn&#39;t make sense.
                        I&#39;ll clarify. I tend to speak pretty fast</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p>&nbsp;No, I think it made sense. Okay, so I think
                        earlier you also mentioned that you joined the Ideas Lab. And I saw that you did research in the VR area. So
                        can you explain what type of research you did? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah, the paper is actually already out. I&#39;ll talk
                        about like what they did because I actually, I stopped taking part in things happened and whatever. So
                        basically the dream of that project was that like, let&#39;s say that we&#39;re in a voice room, right? So
                        we&#39;re doing Zoom like this. And let&#39;s say that we&#39;re not very comfortable showing our face, but
                        we have an avatar, like we have like the Apple Animoji, right? So let&#39;s say that when I&#39;m speaking,
                        right? Now that I&#39;ve learned how I&#39;ve learned, this is how I look when I&#39;m mad, this is how I
                        look when I&#39;m surprised, things like that, I&#39;m able to just take in my language, right? My actual
                        speech and figure out what my face would look like at the time. So the whole point of this is that like, if
                        I&#39;m in VR, right, and I don&#39;t have access to compute or like track my face purely, I&#39;m able to
                        turn in my speech to an expressive talking head pretty much. So we call it speech to effective gestures. But
                        what that means is that like, even if I&#39;m just speaking, you can figure out what my emotions like,
                        right? And if I had a 3D like a thing about a 3D cloud of like a human face, right? Think about Snapchat
                        filters. When I&#39;m angry, right? It&#39;s able to figure out what my face looks like at that time. So
                        that&#39;s what we&#39;re trying to figure out</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> So it tries to like incorporate the semantic meaning
                        behind your sentences, and it just maps out onto like a avatar</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;Yeah, but not only that, right? I&#39;m also
                        speaking, the avatar speaking, right? So first off, it has to learn how mouth movements work, one, but also
                        how mouth movements and facial expressions combine. </p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Oh, so does the model also take into account maybe the
                        volume of your voice? So like, maybe if I&#39;m, I don&#39;t know, like, I can&#39;t really think of an
                        example, but maybe if I&#39;m loud at one point, that might lead to a different expression as opposed to
                        when I&#39;m, you know, whispering or something?</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;Yeah, actually, there&#39;s a thing called VAD in
                        the emotional space. I forget what the D stands for, but one is for valence and arousal. So you could think
                        of like the arousal was like how energetic this level is, right? So if I&#39;m very angry and I&#39;m super
                        loud, my arousal is pretty high, right? So like, part of like when you express emotion is that like,
                        obviously, you&#39;re going to classify what&#39;s your emotion, whether mine might be inquisitive, yours
                        might be like questioning or to use like a common emotion angry, right? So even though I classify that,
                        right, I have to figure out that like, based off this like sound snippet, what is like the face look like to
                        go extra loud or things like that, right? That&#39;s part of your training data.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so it takes that into account the VAD? What did
                        you say? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;VAD is just an example of what I&#39;m saying. In
                        our example, like, the whole idea is that we synthesize speech in the motion, right? And then once we
                        synthesize speech and motion, let&#39;s say our training data with someone speaking and then their audio
                        file, right? So when someone&#39;s speaking, obviously your face changes when you&#39;re like making more
                        noise, right? So you could think of it that like the facial motion that&#39;s expressed with that is now
                        associated with that sound bite. But on top of that, right, you also have a label for what emotion that
                        is</p>
                <p>So when I&#39;m speaking loud and I&#39;m saying that word and I&#39;m angry, it
                        looks different than when I&#39;m speaking loud and I&#39;m explaining how angry I am to you.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, wait, so, so from what I&#39;m gathering, the
                        algorithm that you worked on, it tried to predict your, you know, some facial expression of an avatar based
                        on how you speak and not the words themselves. So like, if you said the word, I&#39;m not really sure. Like,
                        let&#39;s say that I said something angry, like a sentence that would indicate that I&#39;m angry, when I
                        talk to it, what I said, like, as if I was passive, then the face would look passive.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah, you can think of it is that like, if you close
                        your eyes and you hear me talk, now that we&#39;ve talked for a little, you can imagine what my face looks
                        like at the time, right? So you&#39;re imagining that you close my eyes. And let&#39;s say you imagine your
                        mom yelling at you, right? You know exactly what your mom&#39;s face looks like. And so the whole idea is
                        that like, if I could synthesize what you look like, when you speak with this assumed emotion, right? Then I
                        could make a better construction of like a realistic person speaking. The whole point of the emotion is that
                        as people, right, when we conversate, we don&#39;t speak monotone, right? It&#39;s a it&#39;s a hard way to
                        actually express ideas without using emotion. And so our intuition was that if we feed an emotion to the
                        context of generating a face that speaks, right, you get a much. more realistic output</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Yeah, I mean, that&#39;s really interesting. Wait, so
                        when you actually train your algorithm, is it like, you like, somebody says a sentence, and then there&#39;s
                        like some sort of like camera or something like showing the exact 3D coordinates of like the outline of your
                        face. And then you just feed that into the model.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah, there&#39;s actually, so like, I can explain, at
                        least when I looked at it, this paper has been worked for like six months after I left. So they did a lot of
                        stuff that was different than when I was saw it. But I can explain to you what I understood at the time. The
                        way that it looked like to me was that like, let&#39;s say the training data is like a bunch of people that
                        have their facial emotions, like corner parts of their whole face tracked, right? And when they&#39;re
                        speaking, yes, we have this audio clip, but also we have a label for what it looks like so that this is an
                        angry person saying a sentence. And this is what their face looked like at every single second of that clip,
                        right? Meaning that like, this is what their whole like thing about the Snapchat facial filter where it has
                        like a whole polygon, this is what the polygons look like at every moment. So the whole idea is that
                        you&#39;re not really reconstructing a person&#39;s face, but that polygon, because you map those those
                        meshes and like in VR in any sort of game, right? Your your meshes was actually mapped on top of the
                        skeleton structure. So you can imagine that we&#39;re just predicting what an average face would speak at
                        that time. And then you map stuff on top of it. </p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> So I mean, I can see how this can have any
                        applications or like VR. So I was going to ask, what is your opinion on the metaverse? Like, is that gonna,
                        you know, pan out? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> I hate to say I have a crystal ball because I
                        don&#39;t, I predicted wrong a lot of times. But I do think that it&#39;s a little bit silly not to imagine
                        that our lives aren&#39;t going to get more integrated with computing as life goes on. I mean, before, what
                        was the bar for like AI, like, okay, I would never be able to beat somebody at chess, that&#39;s a long gone
                        game, right? But never be able to beat somebody at Go, that&#39;s that&#39;s gone. It will never be able to
                        drive a car. I mean, I work at a company that literally challenges that, right? And then now it&#39;s like,
                        it&#39;ll never be able to write essays and the bar for essays has now risen. It&#39;s a moving game where
                        like, I don&#39;t know what I define is like, we&#39;re only gonna, we&#39;re gonna live real life SAO
                        pretty much, right? If anyone out there watches this anime. I don&#39;t know if that&#39;s how I describe
                        it, but a lot more of your life will be interacting with different forms. Let me rephrase this. A lot more
                        of your life will be interacting online, right? And so however we can make that more expressive and more, I
                        guess, core to the human like emotion, I think that&#39;ll be very valuable as time goes on</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so what made you interested in AI and what kind
                        of resources did you use to learn more about it at Purdue? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Good question. I&#39;m interested in AI, but I would
                        also call myself a generalist</p>
                <p>I don&#39;t think I&#39;m like a superstar in any specific field. I just like to
                        learn a lot. What I think helped me a lot is honestly, I, I bother a lot of people like I ask, I like ask a
                        lot of people like, Hey, I want to learn this, I&#39;m doing this, do you have any tips for it? Let&#39;s
                        say for example, I wanted to learn more about doing a research skill project, right? I would, let&#39;s say
                        at this time, I tried a couple of tutorials, I made a couple of networks, I like the stuff I want to take it
                        more seriously. I would shoot emails to professors saying, I like this, what you do is similar</p>
                <p>Let&#39;s say what Professor Bera, the ideas lab, he works on turning human like
                        emotion into like a the AI and robotic space. I like those things are worked on human expression into, in a
                        like a computer vision product, meaning that I would interact with my real world and I turned that into like
                        a something in the computing space, it&#39;s called AirDraw.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p>&nbsp;Airdraw. Oh. I think I saw a video, it&#39;s like
                        where you just like you draw in the air </p>
                <p>(here is video: <a class="c6"
                            href="https://arefmalek.github.io/blog/Airdraw/">https://arefmalek.github.io/blog/Airdraw/</a>)</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> I made that. But anyways, what&#39;s important about
                        that is that like, I always just reached out and then I said, Hey, I want to learn a bunch. Do you have any
                        time for me? And I try to find my way there. Resources that I recommend to everyone, I would recommend that
                        you just start to read more, meaning that you don&#39;t have to understand 100% get to like 50%, 70%, 80%. I
                        would use stuff like d2l ai. I would use stuff like Andrej Karpethy. </p>
                <p>(He has a youtube channel)</p>
                <p>He&#39;s really brilliant. I just love to listen to him speak and just teach. And
                        also like Purdue, I would say Purdue curriculum is trending more towards AI. That&#39;s why we have a whole
                        major for it now. But a lot of the courses will help you gain foundational knowledge or at least understand
                        where the history of AI was</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Yeah, yeah, I mean, I know that AI majors, they need
                        to take like some philosophy courses or something</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah. But let&#39;s say that it&#39;s a very broad
                        question</p>
                <p>&nbsp;</p>
                <p>*** Weird noise so had to cut out</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> I&#39;ll say like this. Let&#39;s say imagine I&#39;m a
                        freshman student, no way I experience, I really like ML@Purdue. And let&#39;s say I don&#39;t think I&#39;m
                        like qualified enough to join an ML project here</p>
                <p>How would I make myself a little bit more presentable or try to improve my chances? I
                        would say first off, with your math skills, what can you learn and what can you make, right? So let&#39;s
                        say that at first I was able to make, I was able to follow a couple of tutorials, I learned a bit of Dumpi,
                        then I learned PyTorch, I made a couple of CNNs, maybe I made an LSTM, I made a couple of networks, right? I
                        learned how back propagation works, I learned how to make stuff, right? That&#39;s considered AI. Now, if I
                        have that knowledge, could I do it at a little bit of a higher level than tutorial, right? The resources
                        aren&#39;t real available to me, but could I learn given some mentorship of like a lab or something? I would
                        do that, I would reach out to a lab and say, hey, I&#39;m starting out, I really want to learn and I like
                        what you guys do for X, Y, and Z</p>
                <p>I noticed that you guys probably need help with this, could I help? I would try to do
                        that for a while. And then once I realized that I can thoroughly understand the baseline, which is like
                        tutorial level, I understand what those things are doing. And then if I was doing research, I understand
                        that these requirements come, then it just shows that I&#39;m someone who can learn, right? I don&#39;t
                        master computer vision, I don&#39;t consider myself a master at all. But I think I can learn, right? If the
                        situation comes down to it. And I think that&#39;s the most important thing you can show on a club app, a
                        job app, research application, anything, right?</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Yeah, so you just mentioned research. So would you say
                        that it&#39;s easy to reach out to professors here for research opportunities? </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yes and no. Certain professors are very open to
                        accepting new people, certain professors aren&#39;t. For example, Professor Bera&rsquo;s lab has ballooned
                        from like 10 to 15 students to like 30 to 40. Their students are pretty much always looking for someone else
                        to come in. And if they&#39;re not, I mean, there&#39;s different professors that are very open to students
                        as well. There&#39;s a whole blossoming of AI in the past year or so. So I would say that like, if you have
                        a way to say like, hey, I know that you&#39;re working in AI, I have these skills, I&#39;d like to learn a
                        little bit more. Could you take me in? It&#39;s, I&#39;d be hard pressed to say that the entire school would
                        say no to you</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Okay, so my final question. So this is sort of a
                        general open-ended question</p>
                <p>How do you think AI will shape our world? It&#39;s a very broad question. </p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah. That&#39;s a very, very broad question.
                        That&#39;s like saying like, how will computer science change the world? It&#39;s, so I will answer it from
                        how, if I was asking this question, what I would think it means, and then I&#39;ll try to answer that. So I
                        was asking this question, I would imagine like, how will AI shape, let&#39;s say the face of CS, the face of
                        the tech industry, and also how everyday people interact with their computers. Let&#39;s just keep it
                        simple, right? So at the base level, I would say that the way I would describe the whole AI trend is kind of
                        no different than any of the other trend that&#39;s occurring right now. Let&#39;s think back like three
                        years ago, right? And let&#39;s say that people thought that banks were going to die out, we&#39;re going to
                        have blockchain instead. Yeah, like cryptocurrency was going to be the new US dollar or something like that,
                        right? This was definitely things that people said, right? And now when you open LinkedIn, you&#39;re going
                        to hear like at least five people in a row say like, AI won&#39;t replace you, of course, they&#39;re using
                        AI will, and they&#39;ll all say the same thing, right? And so I would say from that point of view, as
                        anyone that&#39;s thinking about like their personal investment, it&#39;s another tool that&#39;s being
                        created, right? Obviously, you&#39;ll have to advance. I&#39;m sure someone that says I&#39;m trying to look
                        for a job, but I don&#39;t know how to use Google will struggle, right? And so the same will be true with
                        chat GPT, with all the other LLM tools and stuff like that</p>
                <p>But what I wouldn&#39;t fall into is I wouldn&#39;t be one of those people that&#39;s
                        going to say like our robot overloads are just bound to come in and take us over, right? That I think
                        we&#39;re a bit away from. And the reason why is because I&#39;m hesitant to trust any sort of doom and
                        gloom approach. I think they&#39;re trying to profit off you at the end of the day. That&#39;s what I&#39;d
                        say. So someone that&#39;s worried about themselves, because I am too, right? I&#39;m sure that a lot of the
                        work that I do, if it&#39;s not in pure research, will eventually become replaceable. That&#39;s a sign that
                        as a humanity, we&#39;re growing, right? If in like, I hope in 10 years, I won&#39;t need to know React.
                        That&#39;s a sign that the field hasn&#39;t like advanced at all. You know? So I would say that like as
                        someone who&#39;s like worried about the advancement of technology, it&#39;s part of the course. I mean, we
                        automate people all the time. When the car was invented, a lot of people lost their jobs. Like a lot of jobs
                        were to take care of horses. We don&#39;t need horses anymore. We have a car, right? But that&#39;s bound to
                        occur. And it&#39;s a little sad to know that it&#39;s bound to happen. But also it&#39;s part of the
                        course. Now, what&#39;s the second way I would talk about this? What does that mean for someone interacting
                        with their computer, right? The way that I like, we look at people use computers and like the 50s and 60s in
                        NASA, when like the moon landings were happening versus like now are totally different. I mean, the
                        calculator has more power than the first rocket to the moon. That&#39;s, you know, and what does that mean?
                        That means that at every scale of technology is about to change. Basically, every single company is racing
                        to figure out like what&#39;s the best hardware design to train LLMs. Like I said before, it&#39;s publicly
                        stated this isn&#39;t like insider knowledge. Tesla had self-proclaimed top 10 supercomputers in the world
                        just to train an AI that just sees things. But now you can imagine in order to train a chat GPT or any sort
                        of model like that, every single query you send to Chat GPT uses eight NVIDIA GPUs on average. That&#39;s
                        like the stat that&#39;s thrown around. So you can imagine that like if it&#39;s a one query per person to
                        eight GPUs, GPUs are expensive. That&#39;s not going to last forever. Every company is racing even at the
                        pure hardware silicon level</p>
                <p>Like how do we make a new design just like handle this, right? That&#39;s why
                        Tesla&#39;s stocks skyrocketed because of Dojo. That&#39;s why Google is making things called TPUs.
                        That&#39;s why NVIDIA makes so much money. That&#39;s why AMD is trying to compete with them. That&#39;s the
                        base level. People are making new ML frameworks. NVIDIA obviously has CUDA. That&#39;s like what they run
                        while processing other GPUs. I&#39;m sure that Tesla&#39;s going to make something</p>
                <p>I don&#39;t know, but they might AMD is bound to make something because they want to
                        compete. Apple&#39;s going to do the same thing. Basically at every single level to handle this new disrupt
                        in technology, we&#39;re going to change how we design things. And eventually, I think AI will also be
                        something that&#39;s similar to commodity. Let&#39;s think about the iPhone 16 years ago. The iPhone is
                        almost as old as us. So if you think about that, when people first saw it, what do they think that iPhones
                        in 16 years would look like? Do they think it would look like what it does now? Or do they think it&#39;s
                        going to be like, I&#39;m going to open my eyes and I see Jarvis around me? Let&#39;s be honest. They
                        probably imagine the second. I think what we tend to forget is that at the end of the day, technology has a
                        financial incentive. So however fast it can be a commodity is probably where it&#39;s going to head. And it
                        probably won&#39;t leave. That&#39;s why Google is like the original algorithm that Google runs can never
                        run the whole internet today. All this SEO stuff that people figured out will make it pretty much
                        inaccessible. So what I&#39;m getting at is that as someone that thinks about how they interact with their
                        computer, I think what you&#39;re failing to realize is that whatever commoditization we find for AI, well,
                        at the end of the day, be a product. I mean, we&#39;re America, we&#39;re a very capitalist place. So
                        we&#39;re trying to find a way to make this usable for people as a product, but also how do we actually meet
                        our goals, aspirations, career goals, finance goals, whatever. Does that answer the question? It&#39;s a
                        very open-ended question. So I&#39;m sorry that I gave an open ended answer.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> No, That was the point. I wanted everyone I
                        interviewed, if I asked them that question, everyone will have completely different answers. So yeah, that
                        was great. So yeah, thank you so much for allowing me to interview you Aref.</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p> Yeah, it&#39;s my pleasure. Appreciate you doing this
                        service pretty much. Some of the smartest people I know are interviewing with you, and I&#39;m just really
                        excited to hear what they say. Yeah</p>
                <p>&nbsp;</p>
                <p class="font-bold">Brian:</p><p> Well, I just like cool stuff, and I&#39;d love to hear
                        more</p>
                <p>&nbsp;</p>
                <p class="font-bold">Aref:</p><p>&nbsp;Yeah, cool</p>
                <p>&nbsp;</p>
                <p>&nbsp;</p>
            </div>
        </ContentSection>
    </article>
    <Footer />
  </body>
</html>
